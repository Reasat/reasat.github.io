<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Projects &amp; Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Projects &amp; Research</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="resume.html">Resume</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Projects &amp; Research</h1>
</div>

<p>
Representative projects spanning production ML systems, Document AI, open-source benchmarks, and biomedical ML.
</p>

<h2>Production ML systems (AWS)</h2>

<h3>Document AI: Fax processing (Ambry Genetics)</h3>
<ul>
<li><p>Built a Temporal-orchestrated, human-in-the-loop fax document processing system on AWS to automate splitting, classification, and patient-to-order matching; reduced manual processing time by 80&#8211;90% and supported 300&#8211;475 faxes/day.</p></li>
<li><p>Implemented Temporal worker services (background processes): heavier workers for OCR/LLM inference and lighter workers for orchestration and workflow bookkeeping.</p></li>
<li><p>OCR + extraction with AWS Textract + AWS Bedrock; workflow state persisted in Amazon Aurora PostgreSQL.</p></li>
</ul>

<h3>Document AI: Insurance denial extraction (Ambry Genetics)</h3>
<ul>
<li><p>Designed an LLM-based system to extract structured fields from insurance denial documents, enabling potential recovery of ~ $3M/year in lost claims.</p></li>
</ul>

<h3>Clinical genomics ML (Ambry Genetics)</h3>
<ul>
<li><p>Built ontology-aware features and trained an XGBoost model to prioritize genes; deployed to EC2 and reduced manual variant analysis workload by 50%.</p></li>
<li><p>Built an LLM-based pedigree image classifier on AWS Bedrock; 99% accuracy on an internal test dataset.</p></li>
</ul>

<h2>Open-source datasets &amp; benchmarks (Bengali.AI)</h2>
<p>
[<a href="https://www.bengali.ai/projects">Full project list</a>]
</p>

<h3>OOD Speech (ASR)</h3>
<ul>
<li><p>First Bengali out-of-distribution speech recognition benchmark: 1100+ hours from 22,000+ contributors across 17 domains; fine-tuned GPT-based Whisper for regional Bengali ASR.
[<a href="https://arxiv.org/abs/2305.09688">paper</a>] [<a href="https://www.kaggle.com/competitions/bengaliai-speech/">Kaggle</a>] [<a href="https://huggingface.co/spaces/bengaliAI/regional_bengali-asr_tugstugi_whisper-medium">demo</a>]</p></li>
</ul>

<h3>Document layout analysis (DocAI)</h3>
<ul>
<li><p>BaDLAD: 33,695 annotated Bengali document samples across six domains; trained Mask R-CNN and YOLO-based object detectors for layout analysis.
[<a href="https://link.springer.com/chapter/10.1007/978-3-031-41676-7_19">paper</a>]</p></li>
</ul>

<h3>Text normalization &amp; parsing</h3>
<ul>
<li><p>Abugida Unicode normalizer/parser libraries supporting 7 Indic languages; improved LLM robustness under adversarial conditions by 5&#8211;10 points across multiple metrics.
[<a href="https://aclanthology.org/2024.lrec-main.1479/">paper</a>]</p></li>
</ul>

<h2>Biomedical ML (Vanderbilt University)</h2>

<h3>Data-efficient self-supervised learning (Histopathology)</h3>
<ul>
<li><p>Built an active learning-based training pipeline for SimCLR; reduced labeled data requirements by 93% and training time by 62%.
[<a href="https://www.sciencedirect.com/science/article/pii/S2666827024000537">paper</a>] [<a href="https://github.com/Reasat/data_efficient_cl">code</a>]</p></li>
</ul>

<h3>MRI segmentation (Soft tissue tumors)</h3>
<ul>
<li><p>Curated a musculoskeletal soft tissue tumor segmentation dataset (199 patients) and built multimodal UNet + SAM models; achieved Dice 80% (SOTA).
[<a href="https://www.arxiv.org/abs/2409.03110">paper</a>] [<a href="https://github.com/Reasat/mstt/">code</a>]</p></li>
</ul>

</td>
</tr>
</table>
</body>
</html>


